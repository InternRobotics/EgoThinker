dataset_path: anno/egotaskqa.json # The name of the dataset as listed by HF in the datasets Hub.
dataset_kwargs:
  token: True
  cache_dir: /mnt/petrelfs/share_data/peibaoqi/egotaskqa/qa_videos
  video: True
task: "egotaskqa" # The name of the task, this should be registered in the task manager. If successful, you can call lmms_eval with this task name by setting `--tasks mme`.
test_split: test # The split of the dataset to use as the test split.
output_type: generate_until # The type of model output for the given task. Options are `generate_until`, `loglikelihood`, and `multiple_choice`.
doc_to_visual: !function utils.egotaskqa_doc_to_visual # The function to process a sample into the appropriate input for the model. 
doc_to_text: !function utils.egotaskqa_doc_to_text # The function to process a sample into the appropriate target output for the model.
doc_to_target: "answer" # The function to process a sample into a list of possible string choices for `multiple_choice` tasks.
generation_kwargs: # Auxiliary arguments for the `generate` function from HF transformers library. This would be used in different models files.
  max_new_tokens: 16
  temperature: 0
  top_p: 1.0
  num_beams: 1
  do_sample: false
# The return value of process_results will be used by metrics
process_results: !function utils.egotaskqa_process_results
# Note that the metric name can be either a registed metric function (such as the case for GQA) or a key name returned by process_results
# e.g. Following metrics `mme_perception_score` is custom defined. 
# So `mme_process_results` function should return the dict `{"mme_perception_score": {sub_k:sub_v, ..., } }`
# And the `mme_aggregate_results` function could get the dict `{sub_k:sub_v, ..., }`, and use the information to gather the final accuracy.
lmms_eval_specific_kwargs:
  root_path: /mnt/petrelfs/share_data/peibaoqi/egotaskqa/qa_videos
metric_list:
  - metric: egotaskqa_accuracy 
    aggregation: !function utils.egotaskqa_aggregate_results # The name of the aggregation function to use for evaluation.
    higher_is_better: true # Whether the metric is better when the value is higher.
